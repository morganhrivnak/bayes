---
title: "biol792ch13hw"
author: "morgan hrivnak"
date: "2024-11-13"
output:
  html_document: default
  pdf_document: default
---
13E1. 
a will have more shrinkage because of the more narrow distritbution causing it to have more informative priors.

13E2. 
y ~ Binomial (1, p)
logit(p) = agroup + Bx
agroup ~ Normal(a,σ)
a ~ Normal(0,2)
σ ~ Exponential(1)
B ~ Normal(0,1)

13E3.
y ~ Normal (mu, sigma)
logit(p) = agroup + Bx
agroup ~ Normal( a, sigmaa)
a ~ Normal (0,2)
sigmaa ~ Exponential(1)
B ~ Normal(0,1)
sigma ~ HalfCauchy(0,2)

13E4.
y ~ Poisson(lambda)
log(lambda) = agroup +Bx
agroup ~ Normal(mu, sigma)
mu ~ Normal(0,10)
sigma ~ Exponential(1)
B ~ Normal(0,5)

13E5.
y ~ Poisson(λ)
log(λ) = agroup + λday + Bx
agroup ~ Normal(mu, sigma)
a ~Normal(0,10)
sigmaa ~Exponential(1)
λday ~ Normal(λ, sigma)
λ ~ Normal(0,10)
sigmaλ ~ Exponential(1)
B ~ Normal(0,5)

13M1.

```{r}
library(rethinking)

data(reedfrogs)
d <- reedfrogs
dat <- list(
  S = d$surv,
  n = d$density,
  tank = 1:nrow(d),
  pred = ifelse(d$pred == "no", 0L, 1L),
  size_ = ifelse(d$size == "small", 1L, 2L)
)

#tank only model

m_Tank <- ulam(
  alist(
    S ~ dbinom(n, p),
    logit(p) <- a[tank],
    a[tank] ~ dnorm(a_bar, sigma),
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dexp(1)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE, iter = 2e3
)

#predation model

m_Pred <- ulam(
  alist(
    S ~ dbinom(n, p),
    logit(p) <- a[tank] + bp * pred,
    a[tank] ~ dnorm(a_bar, sigma),
    bp ~ dnorm(-0.5, 1),
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dexp(1)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE, iter = 2e3
)

#size model

m_Size <- ulam(
  alist(
    S ~ dbinom(n, p),
    logit(p) <- a[tank] + s[size_],
    a[tank] ~ dnorm(a_bar, sigma),
    s[size_] ~ dnorm(0, 0.5),
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dexp(1)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE, iter = 2e3
)

#predation and size model
m_Additive <- ulam(
  alist(
    S ~ dbinom(n, p),
    logit(p) <- a[tank] + bp * pred + s[size_],
    a[tank] ~ dnorm(a_bar, sigma),
    bp ~ dnorm(-0.5, 1),
    s[size_] ~ dnorm(0, 0.5),
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dexp(1)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE, iter = 2e3
)

#plotting everything together
plot(coeftab(m_Tank, m_Pred, m_Size, m_Additive),
  pars = "sigma",
  labels = c("Tank", "Predation", "Size", "Additive")
)

```
When we do not include predation in our models a lot of the variation we see then goes to the tank variable. Predation is the cause of most of the variation that we see and by not including it we can assign this variation to the tank intercepts.

13M2.
```{r}
compare(m_Tank, m_Pred, m_Size, m_Additive)
```

The models generally perform very simliarly so it is difficult to see how they are affected. I am unsure how I would plot this to see the parameters from the posterior...

13M3.
```{r}
m_TankCauchy <- ulam(
  alist(
    S ~ dbinom(n, p),
    logit(p) <- a[tank],
    a[tank] ~ dcauchy(a_bar, sigma),
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dexp(1)
  ),
  data = dat, chains = 4, cores = 4, log_lik = TRUE,
  iter = 2e3, control = list(adapt_delta = 0.99)
)


a_Tank <- apply(extract.samples(m_Tank)$a, 2, mean)
a_TankCauchy <- apply(extract.samples(m_TankCauchy)$a, 2, mean)
plot(a_Tank, a_TankCauchy,
  pch = 16, col = rangi2,
  xlab = "intercept (Gaussian prior)", ylab = " intercept (Cauchy prior)"
)
abline(a = 0, b = 1, lty = 2)
```
The Cauchy prior extremes are even more extreme than the Gaussian prior. One cause for this is how much adaptive shrinkage has occurred. The Cauchy distribution is less concentrated than the Gaussian distribution, showing that the Gaussian estimates have more shrinkage so they are less extreme.

13M4. 
```{r}
m5.3t <- quap(
  alist(
    D ~ dstudent (2, mu, sigma) ,
    mu <- a +bM*M + bA*A ,
    a ~ dnorm (0, 0.2) , 
    bM ~ dnorm (0, 0.5) ,
    bA ~ dnrom (0, 0.5) ,
    sigma ~ dexp(1)
  ) , data = d
)


```
Shrinkage is the reduction parameters towards seo and help improve predictions and estimate regression. It has the potential to show a more accurate model. In this case the Gaussian model is more concentrated than the cauchy distirbution.


13M5.
```{r}
#data for 13M4
data(chimpanzees)
d <- chimpanzees
d$treatment <- 1 + d$prosoc_left + 2 * d$condition
dat_list <- list(
  pulled_left = d$pulled_left,
  actor = d$actor,
  block_id = d$block,
  treatment = as.integer(d$treatment)
)

#from the book
m13.5 <- ulam(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) <- a[actor] + g[block_id] + b[treatment],
    b[treatment] ~ dnorm(0, 0.5),
    ## adaptive priors
    a[actor] ~ dnorm(a_bar, sigma_a),
    g[block_id] ~ dnorm(0, sigma_g),
    ## hyper-priors
    a_bar ~ dnorm(0, 1.5),
    sigma_a ~ dexp(1),
    sigma_g ~ dexp(1)
  ),
  data = dat_list, chains = 4, cores = 4, log_lik = TRUE
)

#adaptive prior on blocks with lambda
m_M5 <- ulam(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) <- a[actor] + g[block_id] + b[treatment],
    b[treatment] ~ dnorm(0, 0.5),
    ## adaptive priors
    a[actor] ~ dnorm(a_bar, sigma_a),
    g[block_id] ~ dnorm(g_bar, sigma_g),
    ## hyper-priors
    a_bar ~ dnorm(0, 1.5),
    g_bar ~ dnorm(0, 1.5),
    sigma_a ~ dexp(1),
    sigma_g ~ dexp(1)
  ),
  data = dat_list, chains = 4, cores = 4, log_lik = TRUE
)

#comparing the models
precis(m13.5, 2, pars = c("a_bar", "b"))

precis(m_M5, 2, pars = c("a_bar", "b", "g_bar"))

```
The adaptive prior creates an infitine number of combination of values of a_bar and sigma that poorly define the posterior. It appears to be over-parameterised so the intercepts are defnied by varying priors. 

13M6. 
```{r}
m.nn <- ulam(
  alist(
    s ~ dnorm(u, 1), 
    u ~ dnorm(10, 1)
  ), data = dat_list, chains =4
)

m.nt <- ulam(
  alist(
    s ~ dnorm(u, 1),
    u ~ dstudent(2, 10, 1)
  ), data = dat_list, chains =4
)

m.tn <- ulam(
  alist(
    s~ dstudent (2, u, 1) , 
    u ~ dnorm(10,1)
  ), data = dat_list, chains =4
)

m.tt <- ulam(
  alist(
    s ~ dstudent(2, u, 1) ,
    u ~ dstudent (2, 10, 1)
  ), data = dat_list, chains =4
)

precis(m.nn)
precis(m.nt)
precis(m.tn)
precis(m.tt)
```

m.nn has a tight posterior becuase the likelihood and the prior are both normal with small variances. m.tn has a wider posterior than m.nn because the students t-distribution has heavier tails affecting the data more. m.nt  has more uncertainty in u leading to a wider posterior than m.nn. m.tt has the widest posteriuor because the data and the prior have extreme values of u so there is less concentration around the values.
